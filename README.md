# Отчет по БДЗ 2 по дл

*Я большая прибольшая вафля, поскольку многие эксперименты я не рисовал на графиках...*


## Эксперименты

В общем для большого количество экспериментов я не делал графики BLEU или ошибки (кросс энтропии), посколько все мои эксперименты заключались в непродолжительных запусках на 10-15 эпохах, что графики особо и не нужны были.

### Токенизаторы

Изначально я пробовал разные токенизаторы из библиотеки `sentencepiece`. Там есть на выбор `unigram`, `bpe`, `char` и `word` (последние две - это посимвольная и пословестная токенизации соотвественно). Я попробовал различные размеры словарей, но в итоге все равно решил остановиться в __пословной__ токенизации, тем более наш исходный текст уже предобработан для этого. 

Так как токенизация пословестная, то смысла использовать библиотеку `sentencepiece` особо не было, поэтому я взял токенизатор по умолчанию из `pytorch`.

### Модель

Конечно же я на мужика решил сразу работать с трансформерами. Не понравилось мне качество работы рекуренток в прошлой дз. В качестве основного кода был использован туториал с официального сайта `pytorch`: https://pytorch.org/tutorials/beginner/translation_transformer.html

К сожалению, из-за ограниченности в ресурсах, выводить во время обучения на график BLEU было чересчур долго. При этом ошибка кросс валидации не особо сильно коррелирует с BLEU: довольно часто было, что ошибка на валидации монотонно убывала, но при этом если делать промежуточные замеры BLEU, то было видно, что в один момент наступало переобчение.

![alt text](https://sun9-70.userapi.com/impg/t24oIfOUWSBfPUdfAPj9icTxTunGuaPjXSZCDw/hOe2OUhJpZA.jpg?size=1456x634&quality=95&sign=293d026ff721ffe92bdd5b4f823beaee&type=album) 

Из гиперпараметров трансформера я попробовал параметры из туториала (график `default`). То есть это по 3 энкодера и декодера, размер эмбеддингов и скрытого слоя равны 512. Такая модель на тесте дала `BLEU=26.32`

Далее мне стало инетерсно попробовать различные размеры эмбеддинга. Брать размер больше 512 оказалось плохой идеей, посколько на GPU не хватало памяти для обучения такого размера, даже с понижением батч сайза (я не пробовал батч сайз размера меньше 48, поскольку это кринж и в таком случае слишком большой разброс градиента во время обучения). Попробовав размер = 256, получил качество сильно ниже (график `emb_size=256`). И на тесте модель дала `BLEU=25.79`. Так что я вернулся к классическим 512.

Далее я попробовал взять в точности архитектуру из великой статьи `Attention is all you need`: по 6 энкодеров и декодеров с размеров скритого слоя 2048. Обучал я такую модель с батч сайзом = 64. Так как батч сайз маленький, то `lr` я взял `0.00007` (график `avid-darkness-12`). BLEU такая модель выдала `26.97`. 

В дальнейшем оказалось, что это наилучший вариант архитектуры, поэтому я обучил модель и на тренировочной выборке, и на валидационной одновременно (чтобы увеличить выборку). Такое решение в итоге дало мне `BLEU=28.22` на тестовой выборке.

Так же я попробовал поиграться с расписанием шага (к примеру график `CosineAnnealingLR` для одноименного раписания). Но это мягко говоря не увенчалось успехом... По хорошему надо было поробовать `NoamOpt`, но я забыл о его существовании...

## Итог

__Наилучший результат на тесте__: `BLEU=28.22`

![alt text](https://sun9-59.userapi.com/impg/I4HmGB02J8_peYNGseufr1hWu3n3F_qaWbei5Q/tTnLksqRYcs.jpg?size=590x184&quality=95&sign=6be740507ad5216634611f168f2559be&type=album) 

__Наилучшие параметры__:

```
num_encoder_layers: int = 6
num_decoder_layers: int = 6
emb_size: int = 512
nhead: int = 8
dim_feedforward: int = 2048
dropout: float = 0.1

num_epochs: int = 15
lr: float = 0.00007
```
