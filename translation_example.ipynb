{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## all imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ershoff/HSE/dl-hse-text-translation/dl-hse/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import dataclasses\n",
        "import yaml\n",
        "import sacrebleu\n",
        "\n",
        "import typing as tp\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm\n",
        "from timeit import default_timer as timer\n",
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data import get_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclasses.dataclass(init=True)\n",
        "class TranslationConfig:\n",
        "\n",
        "    data_dir: str = 'data/'\n",
        "\n",
        "    test_file: str = 'test1.de-en.de'\n",
        "\n",
        "    src_max_length: int = 96\n",
        "    tgt_max_length: int = 96\n",
        "\n",
        "    unk_id: int = 0\n",
        "    pad_id: int = 1\n",
        "    bos_id: int = 2\n",
        "    eos_id: int = 3\n",
        "\n",
        "    batch_size: int = 96\n",
        "\n",
        "    num_encoder_layers: int = 3\n",
        "    num_decoder_layers: int = 3\n",
        "    emb_size: int = 512\n",
        "    nhead: int = 8\n",
        "    dim_feedforward: int = 2048\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    num_epochs: int = 25\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.src_files: tp.List[str] = [f'{self.data_dir}train.de-en.de', f'{self.data_dir}val.de-en.de']\n",
        "        self.tgt_files: tp.List[str] = [f'{self.data_dir}train.de-en.en', f'{self.data_dir}val.de-en.en']\n",
        "\n",
        "    @classmethod\n",
        "    def from_yaml(cls, raw_yaml: tp.Union[str, tp.TextIO]):\n",
        "        with open(raw_yaml, \"rt\", encoding=\"utf8\") as stream:\n",
        "            data = yaml.safe_load(stream)\n",
        "        data = cls(**data)\n",
        "        return data\n",
        "\n",
        "\n",
        "CONFIG = TranslationConfig()\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyTokenizer:\n",
        "    def __init__(\n",
        "        self, \n",
        "        data_file: str, \n",
        "        max_length: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Dataset with texts, supporting BPE tokenizer\n",
        "        :param data_file: txt file containing texts\n",
        "        :param sp_model_prefix: path prefix to save tokenizer model\n",
        "        :param vocab_size: sentencepiece tokenizer vocabulary size\n",
        "        :param normalization_rule_name: sentencepiece tokenizer normalization rule\n",
        "        :param model_type: sentencepiece class model type\n",
        "        :param max_length: maximal length of text in tokens\n",
        "        \"\"\"\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        with open(data_file) as file:\n",
        "            texts = file.readlines()\n",
        "\n",
        "        self.texts = texts\n",
        "\n",
        "        self.tokenizer = get_tokenizer(None)\n",
        "\n",
        "        special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "        self.vocab_transform = build_vocab_from_iterator(\n",
        "            self._yield_tokens(),\n",
        "            min_freq=1,\n",
        "            specials=special_symbols,\n",
        "            special_first=True\n",
        "        )\n",
        "        self.vocab_transform.set_default_index(CONFIG.unk_id)\n",
        "\n",
        "    def _yield_tokens(self) -> tp.List[str]:\n",
        "        for data_sample in self.texts:\n",
        "            yield self.tokenizer(data_sample)\n",
        "\n",
        "    def __getitem__(self, item: int) -> tp.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        seq = self.texts[item]\n",
        "        tokens = self.tokenizer(seq)\n",
        "        token_ids = self.vocab_transform(tokens)\n",
        "\n",
        "        token_ids = torch.cat((\n",
        "            torch.tensor([CONFIG.bos_id], dtype=torch.int64),\n",
        "            torch.tensor(token_ids, dtype=torch.int64),\n",
        "            torch.tensor([CONFIG.eos_id], dtype=torch.int64)\n",
        "        ))\n",
        "\n",
        "        padded_tokens = torch.ones(self.max_length, dtype=torch.int64) * CONFIG.pad_id\n",
        "        padded_tokens[: len(token_ids)] = token_ids\n",
        "\n",
        "        return padded_tokens\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, \n",
        "        config: TranslationConfig,\n",
        "        *,\n",
        "        split\n",
        "    ):\n",
        "        split = 0 if split == 'train' else 1\n",
        "        self.src_tokenizer = MyTokenizer(\n",
        "            config.src_files[split], \n",
        "            config.src_max_length\n",
        "        )\n",
        "        self.tgt_tokenizer = MyTokenizer(\n",
        "            config.tgt_files[split], \n",
        "            config.tgt_max_length\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_tokenizer.texts)\n",
        "\n",
        "    def __getitem__(self, item: int) -> tp.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.src_tokenizer[item], self.tgt_tokenizer[item]\n",
        "\n",
        "    def text2ids(self, texts: tp.Union[str, tp.List[str]]) -> tp.Union[tp.List[int], tp.List[tp.List[int]]]:\n",
        "        \"\"\"\n",
        "        Encode a text or list of texts as tokenized indices\n",
        "        :param texts: text or list of texts to tokenize\n",
        "        :return: encoded indices\n",
        "        \"\"\"\n",
        "        tokens = self.src_tokenizer.tokenizer(texts)\n",
        "        token_ids = self.src_tokenizer.vocab_transform(tokens)\n",
        "        token_ids = torch.cat((\n",
        "            torch.tensor([CONFIG.bos_id], dtype=torch.int64),\n",
        "            torch.tensor(token_ids, dtype=torch.int64),\n",
        "            torch.tensor([CONFIG.eos_id], dtype=torch.int64)\n",
        "        ))\n",
        "        return torch.tensor(token_ids, dtype=torch.int64)\n",
        "\n",
        "    def ids2text(self, ids: tp.Union[torch.Tensor, tp.List[int], tp.List[tp.List[int]]]) -> tp.Union[str, tp.List[str]]:\n",
        "        \"\"\"\n",
        "        Decode indices as a text or list of tokens\n",
        "        :param ids: 1D or 2D list (or torch.Tensor) of indices to decode\n",
        "        :return: decoded texts\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(ids):\n",
        "            assert len(ids.shape) <= 2, 'Expected tensor of shape (length, ) or (batch_size, length)'\n",
        "            ids = ids.cpu().tolist()\n",
        "\n",
        "        return self.tgt_tokenizer.vocab_transform.lookup_tokens(ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* torch.log(torch.tensor(10000)) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        return self.embedding(tokens.long()) * torch.sqrt(torch.tensor(self.emb_size))\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,\n",
        "                trg: torch.Tensor,\n",
        "                src_mask: torch.Tensor,\n",
        "                tgt_mask: torch.Tensor,\n",
        "                src_padding_mask: torch.Tensor,\n",
        "                tgt_padding_mask: torch.Tensor,\n",
        "                memory_key_padding_mask: torch.Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[1]\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == CONFIG.pad_id)\n",
        "    tgt_padding_mask = (tgt == CONFIG.pad_id)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model: Seq2SeqTransformer, optimizer, train_iter: TextDataset):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=CONFIG.batch_size)\n",
        "\n",
        "    for src, tgt in tqdm(train_dataloader, desc='training'):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_out = tgt[:, 1:]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=CONFIG.pad_id)(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model: Seq2SeqTransformer, val_iter: TextDataset):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=CONFIG.batch_size)\n",
        "\n",
        "    for src, tgt in tqdm(val_dataloader, desc='validation'):        \n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_out = tgt[:, 1:]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=CONFIG.pad_id)(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)\n",
        "\n",
        "\n",
        "def greedy_decode(model: Seq2SeqTransformer, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out # .transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if next_word == CONFIG.eos_id:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate(model: torch.nn.Module, src_sentence: str, text2ids, ids2text):\n",
        "    model.eval()\n",
        "    src = text2ids(src_sentence).view(1, -1)\n",
        "    num_tokens = src.shape[1]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=CONFIG.bos_id).flatten()\n",
        "    return \"\".join(ids2text(tgt_tokens)).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = TextDataset(CONFIG, split='train')\n",
        "dataset_val = TextDataset(CONFIG, split='val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = Seq2SeqTransformer(\n",
        "    num_encoder_layers=CONFIG.num_encoder_layers,\n",
        "    num_decoder_layers=CONFIG.num_decoder_layers,\n",
        "    emb_size=CONFIG.emb_size,\n",
        "    nhead=CONFIG.nhead,\n",
        "    src_vocab_size=len(dataset_train.src_tokenizer.vocab_transform),\n",
        "    tgt_vocab_size=len(dataset_train.tgt_tokenizer.vocab_transform),\n",
        "    dim_feedforward=CONFIG.dim_feedforward,\n",
        "    dropout=CONFIG.dropout\n",
        ").to(DEVICE)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training:   0%|          | 0/1531 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['david', 'gallo', ':', 'das', 'ist', 'bill', 'lange', '.', 'ich', 'bin', 'dave', 'gallo', '.']\n",
            "['david', 'gallo', ':', 'this', 'is', 'bill', 'lange', '.', 'i', \"'m\", 'dave', 'gallo', '.']\n",
            "['wir', 'werden', 'ihnen', 'einige', 'geschichten', 'über', 'das', 'meer', 'in', 'videoform', 'erzählen', '.']\n",
            "['and', 'we', \"'re\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']\n",
            "['wir', 'haben', 'ein', 'paar', 'der', 'unglaublichsten', 'aufnahmen', 'der', 'titanic', ',', 'die', 'man', 'je', 'gesehen', 'hat', ',', ',', 'und', 'wir', 'werden', 'ihnen', 'nichts', 'davon', 'zeigen', '.']\n",
            "['we', \"'ve\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'titanic', 'that', \"'s\", 'ever', 'been', 'seen', ',', 'and', 'we', \"'re\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']\n",
            "['die', 'wahrheit', 'ist', ',', 'dass', 'die', 'titanic', '–', 'obwohl', 'sie', 'alle', 'kinokassenrekorde', 'bricht', '–', 'nicht', 'gerade', 'die', 'aufregendste', 'geschichte', 'vom', 'meer', 'ist', '.']\n",
            "['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'titanic', '--', 'even', 'though', 'it', \"'s\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', 'it', \"'s\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']\n",
            "['ich', 'denke', ',', 'das', 'problem', 'ist', ',', 'dass', 'wir', 'das', 'meer', 'für', 'zu', 'selbstverständlich', 'halten', '.']\n",
            "['and', 'the', 'problem', ',', 'i', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']\n",
            "['wenn', 'man', 'darüber', 'nachdenkt', ',', 'machen', 'die', 'ozeane', '75', '%', 'des', 'planeten', 'aus', '.']\n",
            "['when', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.']\n",
            "['der', 'großteil', 'der', 'erde', 'ist', 'meerwasser', '.']\n",
            "['most', 'of', 'the', 'planet', 'is', 'ocean', 'water', '.']\n",
            "['die', 'durchschnittliche', 'tiefe', 'ist', 'etwa', '3', 'kilometer', '.']\n",
            "['the', 'average', 'depth', 'is', 'about', 'two', 'miles', '.']\n",
            "['ein', 'teil', 'des', 'problems', 'ist', ',', 'dass', 'wir', 'am', 'strand', 'stehen', 'oder', 'bilder', 'wie', 'dieses', 'hier', 'sehen', 'und', 'auf', 'die', 'riesige', 'blaue', 'weite', 'schauen', ',', 'und', 'sie', 'schimmert', 'und', 'bewegt', 'sich', ',', 'es', 'gibt', 'wellen', ',', 'brandung', 'und', 'gezeiten', ',', 'aber', 'wir', 'haben', 'keine', 'ahnung', ',', 'was', 'darin', 'verborgen', 'ist', '.']\n",
            "['part', 'of', 'the', 'problem', ',', 'i', 'think', ',', 'is', 'we', 'stand', 'at', 'the', 'beach', ',', 'or', 'we', 'see', 'images', 'like', 'this', 'of', 'the', 'ocean', ',', 'and', 'you', 'look', 'out', 'at', 'this', 'great', 'big', 'blue', 'expanse', ',', 'and', 'it', \"'s\", 'shimmering', 'and', 'it', \"'s\", 'moving', 'and', 'there', \"'s\", 'waves', 'and', 'there', \"'s\", 'surf', 'and', 'there', \"'s\", 'tides', ',', 'but', 'you', 'have', 'no', 'idea', 'for', 'what', 'lies', 'in', 'there', '.']\n",
            "['in', 'den', 'ozeanen', 'befinden', 'sich', 'die', 'längsten', 'gebirgszüge', 'des', 'planeten', '.']\n",
            "['and', 'in', 'the', 'oceans', ',', 'there', 'are', 'the', 'longest', 'mountain', 'ranges', 'on', 'the', 'planet', '.']\n",
            "['die', 'meisten', 'tiere', 'leben', 'in', 'den', 'ozeanen', '.']\n",
            "['most', 'of', 'the', 'animals', 'are', 'in', 'the', 'oceans', '.']\n",
            "['der', 'großteil', 'der', 'erdbeben', 'und', 'vulkanausbrüche', 'spielt', 'sich', 'im', 'meer', 'ab', '–', 'am', 'meeresboden', '.']\n",
            "['most', 'of', 'the', 'earthquakes', 'and', 'volcanoes', 'are', 'in', 'the', 'sea', ',', 'at', 'the', 'bottom', 'of', 'the', 'sea', '.']\n",
            "['an', 'einigen', 'stellen', 'ist', 'die', 'vielfalt', 'und', 'die', 'dichte', 'des', 'lebens', 'im', 'ozean', 'höher', 'als', 'in', 'den', 'regenwäldern', '.']\n",
            "['the', 'biodiversity', 'and', 'the', 'biodensity', 'in', 'the', 'ocean', 'is', 'higher', ',', 'in', 'places', ',', 'than', 'it', 'is', 'in', 'the', 'rainforests', '.']\n",
            "['das', 'meiste', 'ist', 'unerforscht', ',', 'und', 'doch', 'gibt', 'es', 'schönheiten', 'wie', 'diese', ',', 'die', 'uns', 'fesseln', 'und', 'uns', 'vertrauter', 'mit', 'ihm', 'machen', '.']\n",
            "['it', \"'s\", 'mostly', 'unexplored', ',', 'and', 'yet', 'there', 'are', 'beautiful', 'sights', 'like', 'this', 'that', 'captivate', 'us', 'and', 'make', 'us', 'become', 'familiar', 'with', 'it', '.']\n",
            "['aber', 'wenn', 'sie', 'am', 'strand', 'stehen', ',', 'möchte', 'ich', ',', 'dass', 'sie', 'sich', 'vorstellen', ',', 'an', 'der', 'grenze', 'zu', 'einer', 'unbekannten', 'welt', 'zu', 'stehen', '.']\n",
            "['but', 'when', 'you', \"'re\", 'standing', 'at', 'the', 'beach', ',', 'i', 'want', 'you', 'to', 'think', 'that', 'you', \"'re\", 'standing', 'at', 'the', 'edge', 'of', 'a', 'very', 'unfamiliar', 'world', '.']\n",
            "['wir', 'müssen', 'uns', 'schon', 'einer', 'sehr', 'speziellen', 'technik', 'bedienen', ',', 'um', 'in', 'diese', 'unbekannte', 'welt', 'vorzudringen', '.']\n",
            "['we', 'have', 'to', 'have', 'a', 'very', 'special', 'technology', 'to', 'get', 'into', 'that', 'unfamiliar', 'world', '.']\n",
            "['wir', 'benutzen', 'das', 'unterseeboot', 'alvin', 'und', 'kameras', ',', 'und', 'diese', 'kameras', 'hat', 'bill', 'lange', 'mit', 'hilfe', 'von', 'sony', 'entwickelt', '.']\n",
            "['we', 'use', 'the', 'submarine', 'alvin', 'and', 'we', 'use', 'cameras', ',', 'and', 'the', 'cameras', 'are', 'something', 'that', 'bill', 'lange', 'has', 'developed', 'with', 'the', 'help', 'of', 'sony', '.']\n",
            "['marcel', 'proust', 'hat', 'einmal', 'gesagt', ':', '\"', 'die', 'wahre', 'entdeckungsreise', 'besteht', 'nicht', 'darin', ',', 'dass', 'man', 'neue', 'länder', 'sucht', ',', 'sondern', 'dass', 'man', 'neue', 'augen', 'hat', '.', '\"']\n",
            "['marcel', 'proust', 'said', ',', '\"', 'the', 'true', 'voyage', 'of', 'discovery', 'is', 'not', 'so', 'much', 'in', 'seeking', 'new', 'landscapes', 'as', 'in', 'having', 'new', 'eyes', '.', '\"']\n",
            "['die', 'menschen', ',', 'die', 'mit', 'uns', 'zusammengearbeitet', 'haben', ',', 'gaben', 'uns', 'neue', 'augen', ',', 'nicht', 'nur', ',', 'um', 'das', 'zu', 'sehen', ',', 'was', 'existiert', '–', 'die', 'neuen', 'länder', 'auf', 'dem', 'meeresboden', '–', 'sondern', 'auch', ',', 'um', 'über', 'das', 'leben', 'auf', 'diesem', 'planeten', 'nachzudenken', '.']\n",
            "['people', 'that', 'have', 'partnered', 'with', 'us', 'have', 'given', 'us', 'new', 'eyes', ',', 'not', 'only', 'on', 'what', 'exists', '--', 'the', 'new', 'landscapes', 'at', 'the', 'bottom', 'of', 'the', 'sea', '--', 'but', 'also', 'how', 'we', 'think', 'about', 'life', 'on', 'the', 'planet', 'itself', '.']\n",
            "['das', 'ist', 'ein', 'weichtier', '.']\n",
            "['here', \"'s\", 'a', 'jelly', '.']\n",
            "['es', 'ist', 'einer', 'meiner', 'lieblinge', ',', 'weil', 'es', 'alle', 'möglichen', 'funktionsteile', 'hat', '.']\n",
            "['it', \"'s\", 'one', 'of', 'my', 'favorites', ',', 'because', 'it', \"'s\", 'got', 'all', 'sorts', 'of', 'working', 'parts', '.']\n",
            "['es', 'hat', 'sich', 'als', 'das', 'längste', 'wesen', 'im', 'meer', 'erwiesen', '.']\n",
            "['this', 'turns', 'out', 'to', 'be', 'the', 'longest', 'creature', 'in', 'the', 'oceans', '.']\n",
            "['es', 'wird', 'bis', 'zu', '50', 'meter', 'lang', '.']\n",
            "['it', 'gets', 'up', 'to', 'about', '150', 'feet', 'long', '.']\n",
            "['sehen', 'sie', 'all', 'die', 'unterschiedlichen', 'teile', '?']\n",
            "['but', 'see', 'all', 'those', 'different', 'working', 'things', '?']\n",
            "['so', 'was', 'mag', 'ich', '.']\n",
            "['i', 'love', 'that', 'kind', 'of', 'stuff', '.']\n",
            "['es', 'hat', 'diese', 'fischköder', 'an', 'der', 'unterseite', '.', 'die', 'bewegen', 'sich', 'auf', 'und', 'ab', '.']\n",
            "['it', \"'s\", 'got', 'these', 'fishing', 'lures', 'on', 'the', 'bottom', '.', 'they', \"'re\", 'going', 'up', 'and', 'down', '.']\n",
            "['es', 'hat', 'tentakel', ',', 'die', 'so', 'herumschwirren', '.']\n",
            "['it', \"'s\", 'got', 'tentacles', 'dangling', ',', 'swirling', 'around', 'like', 'that', '.']\n",
            "['es', 'ist', 'eine', 'kolonie', 'von', 'tieren', '.']\n",
            "['it', \"'s\", 'a', 'colonial', 'animal', '.']\n",
            "['das', 'sind', 'alles', 'einzelne', 'lebewesen', ',', 'die', 'sich', 'zu', 'diesem', 'einen', 'organismus', 'zusammenschließen', '.']\n",
            "['these', 'are', 'all', 'individual', 'animals', 'banding', 'together', 'to', 'make', 'this', 'one', 'creature', '.']\n",
            "['es', 'hat', 'vorne', 'diese', 'antriebsdüsen', ',', 'die', 'es', 'gleich', 'benutzt', ',', 'und', 'ein', 'kleines', 'licht', '.']\n",
            "['and', 'it', \"'s\", 'got', 'these', 'jet', 'thrusters', 'up', 'in', 'front', 'that', 'it', \"'ll\", 'use', 'in', 'a', 'moment', ',', 'and', 'a', 'little', 'light', '.']\n",
            "['wenn', 'man', 'alle', 'großen', 'fische', 'und', 'fischschulen', 'und', 'all', 'das', 'nimmt', 'und', 'auf', 'die', 'eine', 'seite', 'der', 'waage', 'legt', 'und', 'alle', 'weichtiere', 'auf', 'die', 'andere', 'seite', ',', 'gewinnen', 'die', 'hier', 'haushoch', '.']\n",
            "['if', 'you', 'take', 'all', 'the', 'big', 'fish', 'and', 'schooling', 'fish', 'and', 'all', 'that', ',', 'put', 'them', 'on', 'one', 'side', 'of', 'the', 'scale', ',', 'put', 'all', 'the', 'jelly-type', 'of', 'animals', 'on', 'the', 'other', 'side', ',', 'those', 'guys', 'win', 'hands', 'down', '.']\n",
            "['der', 'großteil', 'der', 'biomasse', 'im', 'meer', 'besteht', 'aus', 'geschöpfen', 'wie', 'diesem', '.']\n",
            "['most', 'of', 'the', 'biomass', 'in', 'the', 'ocean', 'is', 'made', 'out', 'of', 'creatures', 'like', 'this', '.']\n",
            "['hier', 'ist', 'das', 'x-wing-todes-weichtier', '.']\n",
            "['here', \"'s\", 'the', 'x-wing', 'death', 'jelly', '.']\n",
            "['sie', 'benutzen', 'die', 'biolumineszenz', ',', 'um', 'geschlechtspartner', 'anzulocken', ',', 'beute', 'zu', 'ködern', 'und', 'zur', 'verständigung', '.']\n",
            "['the', 'bioluminescence', '--', 'they', 'use', 'the', 'lights', 'for', 'attracting', 'mates', 'and', 'attracting', 'prey', 'and', 'communicating', '.']\n",
            "['wir', 'hätten', 'niemals', 'die', 'zeit', ',', 'ihnen', 'all', 'unser', 'archivmaterial', 'von', 'den', 'weichtieren', 'zu', 'zeigen', '.']\n",
            "['we', 'couldn', \"'t\", 'begin', 'to', 'show', 'you', 'our', 'archival', 'stuff', 'from', 'the', 'jellies', '.']\n",
            "['es', 'gibt', 'sie', 'in', 'den', 'unterschiedlichsten', 'größen', 'und', 'formen', '.']\n",
            "['they', 'come', 'in', 'all', 'different', 'sizes', 'and', 'shapes', '.']\n",
            "['bill', 'lange', ':', 'wir', 'vergessen', 'leicht', ',', 'dass', 'das', 'meer', 'durchschnittlich', 'mehrere', 'kilometer', 'tief', 'ist', 'und', 'dass', 'wir', 'nur', 'die', 'tiere', 'wirklich', 'kennen', ',', 'die', 'in', 'den', 'oberen', 'hundert', 'metern', 'leben', ',', 'aber', 'was', 'zwischen', 'dort', 'und', 'dem', 'meeresboden', 'lebt', ',', 'ist', 'uns', 'nicht', 'vertraut', '.']\n",
            "['bill', 'lange', ':', 'we', 'tend', 'to', 'forget', 'about', 'the', 'fact', 'that', 'the', 'ocean', 'is', 'miles', 'deep', 'on', 'average', ',', 'and', 'that', 'we', \"'re\", 'real', 'familiar', 'with', 'the', 'animals', 'that', 'are', 'in', 'the', 'first', '200', 'or', '300', 'feet', ',', 'but', 'we', \"'re\", 'not', 'familiar', 'with', 'what', 'exists', 'from', 'there', 'all', 'the', 'way', 'down', 'to', 'the', 'bottom', '.']\n",
            "['diese', 'tierarten', 'leben', 'in', 'einem', 'dreidimensionalen', 'raum', ',', 'einem', 'mikrogravitationsraum', ',', 'den', 'wir', 'überhaupt', 'noch', 'nicht', 'erforscht', 'haben', '.']\n",
            "['and', 'these', 'are', 'the', 'types', 'of', 'animals', 'that', 'live', 'in', 'that', 'three-dimensional', 'space', ',', 'that', 'micro-gravity', 'environment', 'that', 'we', 'really', 'haven', \"'t\", 'explored', '.']\n",
            "['man', 'hört', 'von', 'riesenkalmaren', 'und', 'so', 'etwas', ',', 'aber', 'einige', 'dieser', 'tiere', 'werden', 'bis', 'zu', 'etwa', '50', 'meter', 'lang', '.']\n",
            "['you', 'hear', 'about', 'giant', 'squid', 'and', 'things', 'like', 'that', ',', 'but', 'some', 'of', 'these', 'animals', 'get', 'up', 'to', 'be', 'approximately', '140', ',', '160', 'feet', 'long', '.']\n",
            "['man', 'weiß', 'sehr', 'wenig', 'über', 'sie', '.']\n",
            "['they', \"'re\", 'very', 'little', 'understood', '.']\n",
            "['dg', ':', 'das', 'ist', 'eines', 'von', 'ihnen', ',', 'noch', 'einer', 'unserer', 'lieblinge', ',', 'denn', 'es', 'ist', 'ein', 'kleiner', 'oktopus', '.']\n",
            "['dg', ':', 'this', 'is', 'one', 'of', 'them', ',', 'another', 'one', 'of', 'our', 'favorites', ',', 'because', 'it', \"'s\", 'a', 'little', 'octopod', '.']\n",
            "['man', 'kann', 'ihm', 'tatsächlich', 'durch', 'den', 'kopf', 'gucken', '.']\n",
            "['you', 'can', 'actually', 'see', 'through', 'his', 'head', '.']\n",
            "['und', 'hier', 'wackelt', 'er', 'mit', 'den', 'ohren', 'und', 'steigt', 'sehr', 'anmutig', 'nach', 'oben', '.']\n",
            "['and', 'here', 'he', 'is', ',', 'flapping', 'with', 'his', 'ears', 'and', 'very', 'gracefully', 'going', 'up', '.']\n",
            "['wir', 'finden', 'sie', 'in', 'allen', 'tiefen', 'und', 'sogar', 'in', 'die', 'tiefsten', 'tiefen', '.']\n",
            "['we', 'see', 'those', 'at', 'all', 'depths', 'and', 'even', 'at', 'the', 'greatest', 'depths', '.']\n",
            "['sie', 'sind', 'von', 'wenigen', 'zentimetern', 'bis', 'hin', 'zu', 'ein', 'paar', 'metern', 'lang', '.']\n",
            "['they', 'go', 'from', 'a', 'couple', 'of', 'inches', 'to', 'a', 'couple', 'of', 'feet', '.']\n",
            "['sie', 'kommen', 'bis', 'ans', 'u-boot', 'heran', '–', 'sie', 'kommen', 'mit', 'den', 'augen', 'ans', 'fenster', 'und', 'gucken', 'ins', 'u-boot', '.']\n",
            "['they', 'come', 'right', 'up', 'to', 'the', 'submarine', '--', 'they', \"'ll\", 'put', 'their', 'eyes', 'right', 'up', 'to', 'the', 'window', 'and', 'peek', 'inside', 'the', 'sub', '.']\n",
            "['es', 'ist', 'eine', 'eigene', 'welt', 'in', 'der', 'welt', ',', 'und', 'wir', 'werden', 'ihnen', 'zwei', 'zeigen', '.']\n",
            "['this', 'is', 'really', 'a', 'world', 'within', 'a', 'world', ',', 'and', 'we', \"'re\", 'going', 'to', 'show', 'you', 'two', '.']\n",
            "['in', 'diesem', 'fall', 'bewegen', 'wir', 'uns', 'durch', 'die', 'ozeanmitte', 'nach', 'unten', 'und', 'sehen', 'solche', 'kreaturen', '.']\n",
            "['in', 'this', 'case', ',', 'we', \"'re\", 'passing', 'down', 'through', 'the', 'mid-ocean', 'and', 'we', 'see', 'creatures', 'like', 'this', '.']\n",
            "['das', 'hier', 'ist', 'eine', 'art', 'unterwasser-hahn', '.']\n",
            "['this', 'is', 'kind', 'of', 'like', 'an', 'undersea', 'rooster', '.']\n",
            "['der', 'hier', ',', 'der', 'irgendwie', 'unglaublich', 'förmlich', 'aussieht', '.']\n",
            "['this', 'guy', ',', 'that', 'looks', 'incredibly', 'formal', ',', 'in', 'a', 'way', '.']\n",
            "['und', 'hier', 'ist', 'einer', 'meiner', 'lieblinge', '.', 'was', 'für', 'ein', 'gesicht', '!']\n",
            "['and', 'then', 'one', 'of', 'my', 'favorites', '.', 'what', 'a', 'face', '!']\n",
            "['was', 'sie', 'hier', 'sehen', ',', 'sind', 'im', 'grunde', 'wissenschaftliche', 'daten', '.']\n",
            "['this', 'is', 'basically', 'scientific', 'data', 'that', 'you', \"'re\", 'looking', 'at', '.']\n",
            "['es', 'sind', 'aufnahmen', ',', 'die', 'wir', 'zu', 'forschungszwecken', 'gemacht', 'haben', '.']\n",
            "['it', \"'s\", 'footage', 'that', 'we', \"'ve\", 'collected', 'for', 'scientific', 'purposes', '.']\n",
            "['das', 'ist', 'auch', 'etwas', ',', 'das', 'bill', 'macht', ',', 'wissenschaftler', 'mit', 'den', 'ersten', 'bildern', 'von', 'diesen', 'tieren', 'zu', 'versorgen', ',', 'in', 'ihrer', 'natürlichen', 'umgebung', '.']\n",
            "['and', 'that', \"'s\", 'one', 'of', 'the', 'things', 'that', 'bill', \"'s\", 'been', 'doing', ',', 'is', 'providing', 'scientists', 'with', 'this', 'first', 'view', 'of', 'animals', 'like', 'this', ',', 'in', 'the', 'world', 'where', 'they', 'belong', '.']\n",
            "['sie', 'fangen', 'sie', 'nicht', 'mit', 'einem', 'netz', '.']\n",
            "['they', 'don', \"'t\", 'catch', 'them', 'in', 'a', 'net', '.']\n",
            "['sie', 'sehen', 'sie', 'sich', 'da', 'unten', 'in', 'dieser', 'welt', 'an', '.']\n",
            "['they', \"'re\", 'actually', 'looking', 'at', 'them', 'down', 'in', 'that', 'world', '.']\n",
            "['wir', 'nehmen', 'einen', 'joystick', ',', 'sitzen', 'vor', 'unserem', 'computer', 'an', 'land', ',', 'drücken', 'den', 'joystick', 'nach', 'vorn', 'und', 'fliegen', 'um', 'den', 'planeten', '.']\n",
            "['we', \"'re\", 'going', 'to', 'take', 'a', 'joystick', ',', 'sit', 'in', 'front', 'of', 'our', 'computer', ',', 'on', 'the', 'earth', ',', 'and', 'press', 'the', 'joystick', 'forward', ',', 'and', 'fly', 'around', 'the', 'planet', '.']\n",
            "['wir', 'werden', 'uns', 'den', 'mittelozeanischen', 'rücken', 'ansehen', ',', 'ein', '64.000', 'kilometer', 'langer', 'gebirgszug', '.']\n",
            "['we', \"'re\", 'going', 'to', 'look', 'at', 'the', 'mid-ocean', 'ridge', ',', 'a', '40,000-mile', 'long', 'mountain', 'range', '.']\n",
            "['die', 'durchschnittliche', 'tiefe', 'der', 'gipfel', 'ist', 'etwa', '2,5', 'kilometer', '.']\n",
            "['the', 'average', 'depth', 'at', 'the', 'top', 'of', 'it', 'is', 'about', 'a', 'mile', 'and', 'a', 'half', '.']\n",
            "['jetzt', 'sind', 'wir', 'über', 'dem', 'atlantik', '–', 'das', 'da', 'ist', 'der', 'rücken', '–', ',', 'werden', 'die', 'karibik', ',', 'in', 'mittelamerika', 'überqueren', 'und', 'im', 'pazifik', 'ankommen', ',', 'neun', 'grad', 'nördlicher', 'breite', '.']\n",
            "['and', 'we', \"'re\", 'over', 'the', 'atlantic', '--', 'that', \"'s\", 'the', 'ridge', 'right', 'there', '--', 'but', 'we', \"'re\", 'going', 'to', 'go', 'across', 'the', 'caribbean', ',', 'central', 'america', ',', 'and', 'end', 'up', 'against', 'the', 'pacific', ',', 'nine', 'degrees', 'north', '.']\n",
            "['wir', 'erstellen', 'karten', 'dieser', 'gebirgszüge', 'mit', 'hilfe', 'von', 'schall', ',', 'sonar', ',', 'und', 'das', 'ist', 'einer', 'dieser', 'gebirgszüge', '.']\n",
            "['we', 'make', 'maps', 'of', 'these', 'mountain', 'ranges', 'with', 'sound', ',', 'with', 'sonar', ',', 'and', 'this', 'is', 'one', 'of', 'those', 'mountain', 'ranges', '.']\n",
            "['wir', 'biegen', 'um', 'eine', 'klippe', 'hier', 'rechts', '.']\n",
            "['we', \"'re\", 'coming', 'around', 'a', 'cliff', 'here', 'on', 'the', 'right', '.']\n",
            "['die', 'höhe', 'dieser', 'berge', 'auf', 'beiden', 'seiten', 'des', 'tals', 'ist', 'größer', 'als', 'die', 'der', 'alpen', 'in', 'den', 'meisten', 'fällen', '.']\n",
            "['the', 'height', 'of', 'these', 'mountains', 'on', 'either', 'side', 'of', 'this', 'valley', 'is', 'greater', 'than', 'the', 'alps', 'in', 'most', 'cases', '.']\n",
            "['und', 'es', 'gibt', 'zehntausende', 'von', 'bergen', 'hier', 'draußen', ',', 'die', 'auf', 'keiner', 'karte', 'verzeichnet', 'sind', '.']\n",
            "['and', 'there', \"'s\", 'tens', 'of', 'thousands', 'of', 'those', 'mountains', 'out', 'there', 'that', 'haven', \"'t\", 'been', 'mapped', 'yet', '.']\n",
            "['das', 'ist', 'ein', 'vulkanischer', 'rücken', '.']\n",
            "['this', 'is', 'a', 'volcanic', 'ridge', '.']\n",
            "['wir', 'tauchen', 'immer', 'tiefer', 'und', 'tiefer', '.']\n",
            "['we', \"'re\", 'getting', 'down', 'further', 'and', 'further', 'in', 'scale', '.']\n",
            "['und', 'schließlich', 'kommen', 'wir', 'auf', 'so', 'etwas', 'wie', 'das', 'hier', '.']\n",
            "['and', 'eventually', ',', 'we', 'can', 'come', 'up', 'with', 'something', 'like', 'this', '.']\n",
            "['das', 'ist', 'ein', 'bild', 'unseres', 'roboters', '–', 'er', 'heißt', 'jason', '.']\n",
            "['this', 'is', 'an', 'icon', 'of', 'our', 'robot', ',', 'jason', ',', 'it', \"'s\", 'called', '.']\n",
            "['man', 'kann', 'in', 'einem', 'raum', 'wie', 'diesem', 'sitzen', ',', 'mit', 'einem', 'joystick', 'und', 'einem', 'headset', 'und', 'so', 'einen', 'roboter', 'ohne', 'zeitverzögerung', 'auf', 'dem', 'meeresboden', 'herumfahren', 'lassen', '.']\n",
            "['and', 'you', 'can', 'sit', 'in', 'a', 'room', 'like', 'this', ',', 'with', 'a', 'joystick', 'and', 'a', 'headset', ',', 'and', 'drive', 'a', 'robot', 'like', 'that', 'around', 'the', 'bottom', 'of', 'the', 'ocean', 'in', 'real', 'time', '.']\n",
            "['unter', 'anderem', 'versuchen', 'wir', 'mit', 'unseren', 'partnern', 'bei', 'woods', 'hole', ',', 'diese', 'virtuelle', 'welt', '–', 'diese', 'welt', ',', 'dieses', 'unerforschte', 'gebiet', '–', 'ins', 'labor', 'zu', 'holen', '.']\n",
            "['one', 'of', 'the', 'things', 'we', \"'re\", 'trying', 'to', 'do', 'at', 'woods', 'hole', 'with', 'our', 'partners', 'is', 'to', 'bring', 'this', 'virtual', 'world', '--', 'this', 'world', ',', 'this', 'unexplored', 'region', '--', 'back', 'to', 'the', 'laboratory', '.']\n",
            "['denn', 'wir', 'sehen', 'sie', 'bisher', 'nur', 'stückweise', '.']\n",
            "['because', 'we', 'see', 'it', 'in', 'bits', 'and', 'pieces', 'right', 'now', '.']\n",
            "['wir', 'nehmen', 'sie', 'entweder', 'als', 'geräusch', 'war', 'oder', 'als', 'video', 'oder', 'auf', 'fotos', 'oder', 'mit', 'hilfe', 'von', 'chemischen', 'sensoren', '–', 'doch', 'wir', 'haben', 'die', 'teile', 'bisher', 'nicht', 'zu', 'einem', 'interessanten', 'bild', 'zusammengesetzt', '.']\n",
            "['we', 'see', 'it', 'either', 'as', 'sound', ',', 'or', 'we', 'see', 'it', 'as', 'video', ',', 'or', 'we', 'see', 'it', 'as', 'photographs', ',', 'or', 'we', 'see', 'it', 'as', 'chemical', 'sensors', ',', 'but', 'we', 'never', 'have', 'yet', 'put', 'it', 'all', 'together', 'into', 'one', 'interesting', 'picture', '.']\n",
            "['hier', 'brillieren', 'bills', 'kameras', 'richtig', '.']\n",
            "['here', \"'s\", 'where', 'bill', \"'s\", 'cameras', 'really', 'do', 'shine', '.']\n",
            "['das', 'hier', 'nennt', 'man', 'eine', 'hydrothermale', 'quelle', '.']\n",
            "['this', 'is', 'what', \"'s\", 'called', 'a', 'hydrothermal', 'vent', '.']\n",
            "['und', 'hier', 'sieht', 'man', 'eine', 'wolke', 'von', 'dichtem', 'hydrogensulfidreichem', 'wasser', ',', 'das', 'aus', 'einer', 'vulkanischen', 'längsachse', 'am', 'meeresboden', 'tritt', '.']\n",
            "['and', 'what', 'you', \"'re\", 'seeing', 'here', 'is', 'a', 'cloud', 'of', 'densely', 'packed', ',', 'hydrogen-sulfide-rich', 'water', 'coming', 'out', 'of', 'a', 'volcanic', 'axis', 'on', 'the', 'sea', 'floor', '.']\n",
            "['es', 'wird', 'bis', 'zu', 'etwa', '300', 'grad', 'celsius', 'heiß', '.']\n",
            "['gets', 'up', 'to', '600', ',', '700', 'degrees', 'f', ',', 'somewhere', 'in', 'that', 'range', '.']\n",
            "['das', 'ist', 'also', 'alles', 'wasser', 'unter', 'dem', 'meer', '–', 'zwei', ',', 'drei', ',', 'vier', 'kilometer', 'in', 'der', 'tiefe', '.']\n",
            "['so', 'that', \"'s\", 'all', 'water', 'under', 'the', 'sea', '--', 'a', 'mile', 'and', 'a', 'half', ',', 'two', 'miles', ',', 'three', 'miles', 'down', '.']\n",
            "['wir', 'wussten', 'bereits', 'in', 'den', '60ern', 'und', '70ern', ',', 'dass', 'es', 'vulkanisch', 'ist', '.']\n",
            "['and', 'we', 'knew', 'it', 'was', 'volcanic', 'back', 'in', 'the', \"'\", '60s', ',', \"'\", '70s', '.']\n",
            "['damit', 'hatten', 'wir', 'einen', 'hinweis', ',', 'dass', 'es', 'diese', 'quellen', 'entlang', 'dieser', 'achse', 'gibt', ',', 'denn', 'wenn', 'es', 'vulkanismus', 'gibt', ',', 'dringt', 'wasser', 'in', 'die', 'spalten', 'am', 'meeresboden', ',', 'trifft', 'auf', 'magma', 'und', 'tritt', 'siedend', 'heiß', 'wieder', 'aus', '.']\n",
            "['and', 'then', 'we', 'had', 'some', 'hint', 'that', 'these', 'things', 'existed', 'all', 'along', 'the', 'axis', 'of', 'it', ',', 'because', 'if', 'you', \"'ve\", 'got', 'volcanism', ',', 'water', \"'s\", 'going', 'to', 'get', 'down', 'from', 'the', 'sea', 'into', 'cracks', 'in', 'the', 'sea', 'floor', ',', 'come', 'in', 'contact', 'with', 'magma', ',', 'and', 'come', 'shooting', 'out', 'hot', '.']\n",
            "['uns', 'war', 'nicht', 'bewusst', ',', 'dass', 'es', 'so', 'reich', 'an', 'hydrogensulfiden', 'ist', '.']\n",
            "['we', 'weren', \"'t\", 'really', 'aware', 'that', 'it', 'would', 'be', 'so', 'rich', 'with', 'sulfides', ',', 'hydrogen', 'sulfides', '.']\n",
            "['wir', 'hatten', 'keine', 'ahnung', 'von', 'diesen', 'dingern', ',', 'die', 'wir', 'schornsteine', 'nennen', '.']\n",
            "['we', 'didn', \"'t\", 'have', 'any', 'idea', 'about', 'these', 'things', ',', 'which', 'we', 'call', 'chimneys', '.']\n",
            "['das', 'ist', 'eine', 'dieser', 'hydrothermalen', 'quellen', '.']\n",
            "['this', 'is', 'one', 'of', 'these', 'hydrothermal', 'vents', '.']\n",
            "['300', 'grad', 'heißes', 'wasser', 'tritt', 'aus', 'der', 'erde', '.']\n",
            "['six', 'hundred', 'degree', 'f', 'water', 'coming', 'out', 'of', 'the', 'earth', '.']\n",
            "['auf', 'beiden', 'seiten', 'liegen', 'gebirgszüge', ',', 'die', 'höher', 'als', 'die', 'alpen', 'sind', ',', 'diese', 'gegend', 'hier', 'ist', 'also', 'sehr', 'dramatisch', '.']\n",
            "['on', 'either', 'side', 'of', 'us', 'are', 'mountain', 'ranges', 'that', 'are', 'higher', 'than', 'the', 'alps', ',', 'so', 'the', 'setting', 'here', 'is', 'very', 'dramatic', '.']\n",
            "['bl', ':', 'das', 'weiße', 'ist', 'eine', 'bakterienart', ',', 'die', 'bei', '180', 'grad', 'celsius', 'gedeiht', '.']\n",
            "['bl', ':', 'the', 'white', 'material', 'is', 'a', 'type', 'of', 'bacteria', 'that', 'thrives', 'at', '180', 'degrees', 'c', '.']\n",
            "['dg', ':', 'ich', 'finde', ',', 'eine', 'der', 'größten', 'geschichten', ',', 'die', 'wir', 'gerade', 'auf', 'dem', 'meeresgrund', 'entdecken', ',', 'ist', ',', 'dass', 'das', 'erste', ',', 'das', 'wir', 'nach', 'einem', 'vulkanausbruch', 'am', 'meeresboden', 'wieder', 'finden', ',', 'bakterien', 'sind', '.']\n",
            "['dg', ':', 'i', 'think', 'that', \"'s\", 'one', 'of', 'the', 'greatest', 'stories', 'right', 'now', 'that', 'we', \"'re\", 'seeing', 'from', 'the', 'bottom', 'of', 'the', 'sea', ',', 'is', 'that', 'the', 'first', 'thing', 'we', 'see', 'coming', 'out', 'of', 'the', 'sea', 'floor', 'after', 'a', 'volcanic', 'eruption', 'is', 'bacteria', '.']\n",
            "['lange', 'haben', 'wir', 'uns', 'gefragt', ':', 'wie', 'sind', 'die', 'alle', 'da', 'unten', 'hingekommen', '?']\n",
            "['and', 'we', 'started', 'to', 'wonder', 'for', 'a', 'long', 'time', ',', 'how', 'did', 'it', 'all', 'get', 'down', 'there', '?']\n",
            "['inzwischen', 'haben', 'wir', 'herausgefunden', ',', 'dass', 'sie', 'wahrscheinlich', 'aus', 'dem', 'erdinneren', 'gekommen', 'sind', '.']\n",
            "['what', 'we', 'find', 'out', 'now', 'is', 'that', 'it', \"'s\", 'probably', 'coming', 'from', 'inside', 'the', 'earth', '.']\n",
            "['sie', 'kommen', 'nicht', 'nur', 'aus', 'der', 'erde', '–', 'vom', 'vulkanismus', 'angestoßene', 'biogenese', '–', 'bakterien', 'unterstützen', 'auch', 'diese', 'kolonien', 'des', 'lebens', '.']\n",
            "['not', 'only', 'is', 'it', 'coming', 'out', 'of', 'the', 'earth', '--', 'so', ',', 'biogenesis', 'made', 'from', 'volcanic', 'activity', '--', 'but', 'that', 'bacteria', 'supports', 'these', 'colonies', 'of', 'life', '.']\n",
            "['der', 'druck', 'beträgt', 'hier', '800', 'kg', 'pro', 'quadratzentimeter', '.']\n",
            "['the', 'pressure', 'here', 'is', '4,000', 'pounds', 'per', 'square', 'inch', '.']\n",
            "['2,5', 'bis', '3', 'oder', '5', 'kilometer', 'unter', 'der', 'oberfläche', '–', 'kein', 'sonnenstrahl', 'ist', 'jemals', 'bis', 'hierher', 'vorgedrungen', '.']\n",
            "['a', 'mile', 'and', 'a', 'half', 'from', 'the', 'surface', 'to', 'two', 'miles', 'to', 'three', 'miles', '--', 'no', 'sun', 'has', 'ever', 'gotten', 'down', 'here', '.']\n",
            "['die', 'ganze', 'energie', ',', 'die', 'diese', 'lebewesen', 'antreibt', ',', 'kommt', 'aus', 'dem', 'erdinneren', '–', 'chemosynthese', 'also', '.']\n",
            "['all', 'the', 'energy', 'to', 'support', 'these', 'life', 'forms', 'is', 'coming', 'from', 'inside', 'the', 'earth', '--', 'so', ',', 'chemosynthesis', '.']\n",
            "['und', 'sie', 'sehen', ',', 'wie', 'dicht', 'das', 'leben', 'hier', 'ist', '.']\n",
            "['and', 'you', 'can', 'see', 'how', 'dense', 'the', 'population', 'is', '.']\n",
            "['das', 'hier', 'sind', 'röhrenwürmer', '.']\n",
            "['these', 'are', 'called', 'tube', 'worms', '.']\n",
            "['bl', ':', 'diese', 'würmer', 'haben', 'kein', 'verdauungssystem', '.', 'sie', 'haben', 'keinen', 'mund', '.']\n",
            "['bl', ':', 'these', 'worms', 'have', 'no', 'digestive', 'system', '.', 'they', 'have', 'no', 'mouth', '.']\n",
            "['aber', 'sie', 'haben', 'zwei', 'arten', 'von', 'kiemenstrukturen', '.']\n",
            "['but', 'they', 'have', 'two', 'types', 'of', 'gill', 'structures', '.']\n",
            "['eine', 'für', 'das', 'aufnehmen', 'von', 'sauerstoff', 'aus', 'dem', 'meerwasser', 'und', 'die', 'andere', 'beherbergt', 'diese', 'chemosynthetischen', 'bakterien', ',', 'die', 'die', 'hydrothermale', 'flüssigkeit', '–', 'das', 'heiße', 'wasser', ',', 'das', 'sie', 'aus', 'dem', 'boden', 'schießen', 'sahen', '–', 'in', 'einfache', 'zucker', 'verwandelt', ',', 'die', 'der', 'röhrenwurm', 'verdauen', 'kann', '.']\n",
            "['one', 'for', 'extracting', 'oxygen', 'out', 'of', 'the', 'deep-sea', 'water', ',', 'another', 'one', 'which', 'houses', 'this', 'chemosynthetic', 'bacteria', ',', 'which', 'takes', 'the', 'hydrothermal', 'fluid', '--', 'that', 'hot', 'water', 'that', 'you', 'saw', 'coming', 'out', 'of', 'the', 'bottom', '--', 'and', 'converts', 'that', 'into', 'simple', 'sugars', 'that', 'the', 'tube', 'worm', 'can', 'digest', '.']\n",
            "['dg', ':', 'sie', 'können', 'sehen', '–', 'hier', 'ist', 'eine', 'krabbe', ',', 'die', 'dort', 'unten', 'lebt', '.']\n",
            "['dg', ':', 'you', 'can', 'see', ',', 'here', \"'s\", 'a', 'crab', 'that', 'lives', 'down', 'there', '.']\n",
            "['sie', 'hat', 'ein', 'stück', 'dieser', 'würmer', 'zu', 'fassen', 'gekriegt', '.']\n",
            "['he', \"'s\", 'managed', 'to', 'grab', 'a', 'tip', 'of', 'these', 'worms', '.']\n",
            "['normalerweise', 'ziehen', 'sie', 'sich', 'zurück', ',', 'sobald', 'eine', 'krabbe', 'sie', 'berührt', '.']\n",
            "['now', ',', 'they', 'normally', 'retract', 'as', 'soon', 'as', 'a', 'crab', 'touches', 'them', '.']\n",
            "['oh', '!', 'guter', 'versuch', '.']\n",
            "['oh', '!', 'good', 'going', '.']\n",
            "['sobald', 'also', 'die', 'krabbe', 'sie', 'berührt', ',', 'ziehen', 'sie', 'sich', 'in', 'ihre', 'schalen', 'zurück', ',', 'so', 'wie', 'fingernägel', '.']\n",
            "['so', ',', 'as', 'soon', 'as', 'a', 'crab', 'touches', 'them', ',', 'they', 'retract', 'down', 'into', 'their', 'shells', ',', 'just', 'like', 'your', 'fingernails', '.']\n",
            "['da', 'spielt', 'sich', 'eine', 'ganz', 'komplexe', 'geschichte', 'ab', ',', 'die', 'wir', 'gerade', 'erst', 'zu', 'verstehen', 'beginnen', ',', 'dank', 'der', 'neuen', 'kameratechnologie', '.']\n",
            "['there', \"'s\", 'a', 'whole', 'story', 'being', 'played', 'out', 'here', 'that', 'we', \"'re\", 'just', 'now', 'beginning', 'to', 'have', 'some', 'idea', 'of', 'because', 'of', 'this', 'new', 'camera', 'technology', '.']\n",
            "['bl', ':', 'diese', 'würmer', 'leben', 'in', 'einem', 'echten', 'temperatur-extrem', '.']\n",
            "['bl', ':', 'these', 'worms', 'live', 'in', 'a', 'real', 'temperature', 'extreme', '.']\n",
            "['ihre', 'füße', 'sind', 'etwa', '200', 'grad', 'celsius', 'heiß', 'und', 'ihre', 'köpfe', 'da', 'draußen', 'sind', 'etwa', 'drei', 'grad', 'kalt', ',', 'es', 'ist', 'also', ',', 'als', 'hätten', 'sie', 'ihre', 'hand', 'im', 'kochenden', 'und', 'ihren', 'fuß', 'in', 'eiswasser', '.']\n",
            "['their', 'foot', 'is', 'at', 'about', '200', 'degrees', 'c', 'and', 'their', 'head', 'is', 'out', 'at', 'three', 'degrees', 'c', ',', 'so', 'it', \"'s\", 'like', 'having', 'your', 'hand', 'in', 'boiling', 'water', 'and', 'your', 'foot', 'in', 'freezing', 'water', '.']\n",
            "['so', 'gefällt', 'ihnen', 'das', 'leben', '.']\n",
            "['that', \"'s\", 'how', 'they', 'like', 'to', 'live', '.']\n",
            "['dg', ':', 'das', 'ist', 'ein', 'weibchen', 'dieser', 'wurmart', '.']\n",
            "['dg', ':', 'this', 'is', 'a', 'female', 'of', 'this', 'kind', 'of', 'worm', '.']\n",
            "['und', 'das', 'ist', 'ein', 'männchen', '.']\n",
            "['and', 'here', \"'s\", 'a', 'male', '.']\n",
            "['wie', 'sie', 'gleich', 'sehen', 'werden', ',', 'dauert', 'es', 'nicht', 'lange', ',', 'bis', 'diese', 'beiden', '–', 'der', 'hier', 'und', 'der', ',', 'der', 'da', 'drüben', 'gleich', 'ins', 'bild', 'kommt', '–', 'zu', 'kämpfen', 'anfangen', '.']\n",
            "['you', 'watch', '.', 'it', 'doesn', \"'t\", 'take', 'long', 'before', 'two', 'guys', 'here', '--', 'this', 'one', 'and', 'one', 'that', 'will', 'show', 'up', 'over', 'here', '--', 'start', 'to', 'fight', '.']\n",
            "['alles', ',', 'was', 'sie', 'sehen', ',', 'findet', 'in', 'der', 'finsternis', 'der', 'tiefsee', 'statt', '.']\n",
            "['everything', 'you', 'see', 'is', 'played', 'out', 'in', 'the', 'pitch', 'black', 'of', 'the', 'deep', 'sea', '.']\n",
            "['es', 'gibt', 'hier', 'keine', 'lichtquellen', 'außer', 'denen', ',', 'die', 'wir', 'mitbringen', '.']\n",
            "['there', 'are', 'never', 'any', 'lights', 'there', ',', 'except', 'the', 'lights', 'that', 'we', 'bring', '.']\n",
            "['jetzt', 'geht', \"'\", 's', 'los', '.']\n",
            "['here', 'they', 'go', '.']\n",
            "['auf', 'einem', 'der', 'letzten', 'tauchgänge', 'haben', 'wir', '200', 'arten', 'in', 'diesen', 'bereichen', 'gezählt', '.', '198', 'waren', 'neu', '–', 'neue', 'arten', '.']\n",
            "['on', 'one', 'of', 'the', 'last', 'dive', 'series', ',', 'we', 'counted', '200', 'species', 'in', 'these', 'areas', '--', '198', 'were', 'new', ',', 'new', 'species', '.']\n",
            "['bl', ':', 'eines', 'der', 'größten', 'probleme', 'der', 'biologen', ',', 'die', 'sich', 'mit', 'diesem', 'lebensraum', 'befassen', ',', 'ist', 'das', 'sammeln', 'dieser', 'tiere', '.']\n",
            "['bl', ':', 'one', 'of', 'the', 'big', 'problems', 'is', 'that', 'for', 'the', 'biologists', 'working', 'at', 'these', 'sites', ',', 'it', \"'s\", 'rather', 'difficult', 'to', 'collect', 'these', 'animals', '.']\n",
            "['sie', 'lösen', 'sich', 'auf', 'dem', 'weg', 'nach', 'oben', 'auf', ',', 'die', 'bilder', 'sind', 'also', 'sehr', 'wichtig', 'für', 'die', 'wissenschaft', '.']\n",
            "['and', 'they', 'disintegrate', 'on', 'the', 'way', 'up', ',', 'so', 'the', 'imagery', 'is', 'critical', 'for', 'the', 'science', '.']\n",
            "['dg', ':', 'zwei', 'oktopusse', 'in', 'etwa', 'drei', 'kilometern', 'tiefe', '.']\n",
            "['dg', ':', 'two', 'octopods', 'at', 'about', 'two', 'miles', 'depth', '.']\n",
            "['diese', 'druckverhältnisse', 'finde', 'ich', 'wirklich', 'erstaunlich', ',', 'dass', 'diese', 'tiere', 'in', 'einer', 'tiefe', 'existieren', 'können', ',', 'bei', 'einem', 'druck', ',', 'der', 'groß', 'genug', 'ist', ',', 'um', 'die', 'titanic', 'wie', 'eine', 'leere', 'coladose', 'zu', 'zerquetschen', '.']\n",
            "['this', 'pressure', 'thing', 'really', 'amazes', 'me', '--', 'that', 'these', 'animals', 'can', 'exist', 'there', 'at', 'a', 'depth', 'with', 'pressure', 'enough', 'to', 'crush', 'the', 'titanic', 'like', 'an', 'empty', 'pepsi', 'can', '.']\n",
            "['was', 'wir', 'bisher', 'gesehen', 'haben', ',', 'war', 'aus', 'dem', 'pazifik', '.']\n",
            "['what', 'we', 'saw', 'up', 'till', 'now', 'was', 'from', 'the', 'pacific', '.']\n",
            "['das', 'hier', 'ist', 'aus', 'dem', 'atlantik', '.', 'eine', 'noch', 'größere', 'tiefe', '.']\n",
            "['this', 'is', 'from', 'the', 'atlantic', '.', 'even', 'greater', 'depth', '.']\n",
            "['sie', 'sehen', 'diese', 'garnele', 'den', 'armen', 'kleinen', 'hier', 'belästigen', ',', 'er', 'schlägt', 'sie', 'mit', 'seiner', 'klaue', 'zurück', '.', 'zack', '!']\n",
            "['you', 'can', 'see', 'this', 'shrimp', 'is', 'harassing', 'this', 'poor', 'little', 'guy', 'here', ',', 'and', 'he', \"'ll\", 'bat', 'it', 'away', 'with', 'his', 'claw', '.', 'whack', '!']\n",
            "['gleichzeitig', 'geschieht', 'hier', 'drüben', 'etwas', '.']\n",
            "['and', 'the', 'same', 'thing', \"'s\", 'going', 'on', 'over', 'here', '.']\n",
            "['was', 'sie', 'hier', 'auf', 'dem', 'rücken', 'der', 'krabbe', 'suchen', ',', 'ist', 'nahrung', ',', 'ein', 'sehr', 'seltsames', 'bakterium', ',', 'das', 'auf', 'den', 'rücken', 'all', 'dieser', 'tiere', 'lebt', '.']\n",
            "['what', 'they', \"'re\", 'getting', 'at', 'is', 'that', '--', 'on', 'the', 'back', 'of', 'this', 'crab', '--', 'the', 'foodstuff', 'here', 'is', 'this', 'very', 'strange', 'bacteria', 'that', 'lives', 'on', 'the', 'backs', 'of', 'all', 'these', 'animals', '.']\n",
            "['und', 'diese', 'garnelen', 'versuchen', ',', 'die', 'bakterien', 'von', 'den', 'rücken', 'all', 'dieser', 'tiere', 'abzuernten', '.']\n",
            "['and', 'what', 'these', 'shrimp', 'are', 'trying', 'to', 'do', 'is', 'actually', 'harvest', 'the', 'bacteria', 'from', 'the', 'backs', 'of', 'these', 'animals', '.']\n",
            "['den', 'krabben', 'gefällt', 'das', 'gar', 'nicht', '.']\n",
            "['and', 'the', 'crabs', 'don', \"'t\", 'like', 'it', 'at', 'all', '.']\n",
            "['diese', 'langen', 'fäden', 'auf', 'dem', 'rücken', 'der', 'krabbe', 'bestehen', 'aus', 'stoffen', ',', 'die', 'das', 'bakterium', 'produziert', '.']\n",
            "['these', 'long', 'filaments', 'that', 'you', 'see', 'on', 'the', 'back', 'of', 'the', 'crab', 'are', 'actually', 'created', 'by', 'the', 'product', 'of', 'that', 'bacteria', '.']\n",
            "['die', 'bakterien', 'sorgen', 'also', 'für', 'haarwuchs', 'auf', 'der', 'krabbe', '.']\n",
            "['so', ',', 'the', 'bacteria', 'grows', 'hair', 'on', 'the', 'crab', '.']\n",
            "['im', 'hintergrund', 'sieht', 'man', 'das', 'wieder', '.']\n",
            "['on', 'the', 'back', ',', 'you', 'see', 'this', 'again', '.']\n",
            "['der', 'rote', 'punkt', 'ist', 'der', 'laserstrahl', 'des', 'u-bootes', 'alvin', ',', 'der', 'uns', 'zeigt', ',', 'wie', 'weit', 'wir', 'etwa', 'von', 'den', 'quellen', 'entfernt', 'sind', '.']\n",
            "['the', 'red', 'dot', 'is', 'the', 'laser', 'light', 'of', 'the', 'submarine', 'alvin', 'to', 'give', 'us', 'an', 'idea', 'about', 'how', 'far', 'away', 'we', 'are', 'from', 'the', 'vents', '.']\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, CONFIG.num_epochs + 1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer, dataset_train)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, dataset_val)\n",
        "\n",
        "    print((f\"\"\"\n",
        "        ======================================\n",
        "        Epoch {epoch}: \\n\n",
        "        \\t Train loss: {train_loss:.3f}, \\n\n",
        "        \\t Val loss: {val_loss:.3f}, \\n\n",
        "        \\t Epoch time = {(end_time - start_time):.3f}s, \\n\\n\n",
        "    \"\"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(transformer.state_dict(), 'transformer.model')\n",
        "torch.save(optimizer.state_dict(), 'transformer.opt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/74/3bw80qxj63382vp4tks4kyb80000gn/T/ipykernel_16091/3276991581.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(token_ids, dtype=torch.int64)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "val_src = dataset_val.src_tokenizer.texts\n",
        "val_tgt = dataset_val.tgt_tokenizer.texts\n",
        "\n",
        "text2ids = dataset_train.text2ids\n",
        "ids2text = dataset_train.ids2text\n",
        "\n",
        "translations = [translate(transformer, sentence, text2ids, ids2text) for sentence in val_src]\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(translations, [val_tgt])\n",
        "print(bleu.score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'choices choices choices responsibilities choices choices choices forced forced forced forced forced choices choices choices choices choices choices'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translations[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_tgt[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(CONFIG.test_file, 'r') as input:\n",
        "    with open('solution.txt', 'w') as output:\n",
        "        for sentence in input.readlines():\n",
        "            print(translate(transformer, sentence, text2ids, ids2text), file=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('dl-hse': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "42c165be1c7c29436dbd68da907b2ae1b53686369319918cb7a02c4492aae618"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
